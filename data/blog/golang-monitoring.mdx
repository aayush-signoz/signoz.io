---
title: Monitor Golang Applications
slug: golang-monitoring
date: 2025-11-26
tags: [Go / Golang, Golang Monitoring / Go Monitoring, OpenTelemetry Instrumentation]
authors: [aayush_sharma]
description: In this article, learn about Golang monitoring and how to monitor Golang applications using OpenTelemetry.
image: /img/blog/2021/06/golang_app_monitoring_cover_hc.webp
keywords: [go application monitoring, opentelemetry, golang monitoring, opentelemetry go, go app, golang]
---

Go (Golang) applications are known for their performance, concurrency model, and efficient use of resources, making Go an easy pick to build modern distributed systems. Effective monitoring of a Go application means understanding how goroutines behave under load, how the runtime allocates memory, how long API calls take, and how external dependencies such as databases or caches influence performance.

## Key Points to Monitor in Golang Setup
Go applications have unique runtime behaviours due to their concurrency model, and understanding these behaviours is key to effective monitoring. In this article we have covered a few important metrics that can help you get started with setting up monitoring in your Golang applications, such as the following:
- `Goroutines`: Represents the current number threads managed by the Go runtime.
- `Heap Memory Usage`: Reflects the number of bytes currently allocated on the Go heap, showing how much application memory is actively in use and needs garbage collection management.
- `Request Latency & Throughput`: Latency measures how long a request takes to complete and throughput measures how many requests are processed over time. Together indicating service responsiveness and load capacity.
- `Business-Level Metrics`: These represent application-specific events (e.g., orders created or per-product order volume), quantifying real user actions and business activity rather than just system performance.
- `Tracing & Structured Logging`: Traces show the execution path of a request across functions and services, while structured logs provide contextual log data enriched with identifiers like trace IDs and span IDs.

While the above metrics provide a solid foundation, there are additional signals you can explore depending on your setup requirements:
- `Garbage Collection Pause Times`: See how garbage collection pauses affect your applicationâ€™s response time.
- `Object Allocation Rates`: Measure memory allocation patterns to identify hotspots and reduce garbage collection pressure.
- `Stack Growth`: Monitor goroutine stack expansion to catch excessive memory usage.
- `GOMAXPROCS`: Observe the number of OS threads Go can use concurrently for potential CPU under-utilization.
- `Panic and Error Counts`: Track unexpected crashes and internal errors for early detection of reliability issues.
- `Database & External Service Latency`: Monitor downstream dependencies like SQL queries, Redis, or third-party APIs.
- `CPU Usage`: Measure system-level compute consumption beyond Go runtime metrics.
- `RSS, Disk & Network I/O`: Understand container or VM-level resource pressure that could affect performance.
- `Profiling Endpoints`: Use `/debug/pprof` to capture CPU, memory, and goroutine profiles for deeper analysis.

Monitoring these internal behaviours provides a deeper understanding of your Go application, going beyond standard surface-level metrics like CPU or latency and offering actionable insights into runtime performance, request behaviours, resource usage, and business events.

## Real Question: Best Way to Monitor Golang Applications?
There is no perfect answer to this question considering there are multiple ways to monitor Go services, and each provides different visibility into application health, runtime, and performance. Teams often evolve through these stages over time:
- `Simple Logging`: Useful for recording events and errors within a single environment, but limited in connecting related requests from multiple services.
- `System Monitoring`: Tracks resource usage at the host or container level, such as CPU consumption, memory usage, disk throughput, and network traffic.
- `Go Profiling (pprof)`: Effective for detailed analysis during performance issues, as it provides low-level insight into CPU execution, goroutine scheduling, and more.
- `Custom Metrics`: Allows developers to track domain-specific counters and values. Valuable for business insights, but consistency and standardization can be challenging across services.
- `APM Agent Monitoring (Datadog, New Relic, AppDynamics)`: Provides automated instrumentation and dashboards, limited by proprietary integrations and lacks visibility into Go internals.
- `Distributed Tracing (Jaeger, Zipkin)`: Makes it easier to observe how requests progress through components and services, revealing where processing time accumulates, though not inherently connected with metrics or logs.

## A Standard in the Industry: Prometheus for Metrics
Prometheus is widely used in Go applications for collecting numeric, timestamped metrics such as `counters`, `gauges`, and `histograms`. It works well for tracking the service state over time and is dependable for detecting issues like elevated error rates, memory growth, or request latency changes. For operational health monitoring, it answers questions like:
- Is the service responding as expected?
- Are failures increasing?
- Is a threshold being crossed that requires attention?

### Where Prometheus Falls Short?
Prometheus helps you see what is happening inside a service, but not why itâ€™s happening. It focuses on aggregated application metrics rather than request-level context. Examples:
- You may see that garbage collection paused execution for several milliseconds, but not which inbound request triggered the spike.
- You may observe higher Redis latency, but not which operation or endpoint caused the increase.
- You may notice goroutine growth, but have no link to the specific code path or workload driving it.

Prometheus is strong for detecting changes and trends in system and runtime indicators, but it does not provide the trace-driven causality that connects individual user requests to internal runtime behavior. Tools that incorporate tracing, such as OpenTelemetry paired with a compatible backend, allow you to follow execution flow and uncover source-level reasons for the metrics you observe.

## OpenTelemetry(OTel): Foundation of Modern Monitoring
[OpenTelemetry](https://signoz.io/opentelemetry/) provides a vendor-neutral framework to instrument Go applications, capturing logs, metrics, and traces from both your code and the runtime. This approach allows you to go beyond high-level metrics and understand how requests flow through the system, where time is spent, and how internal behaviours affect performance. By sending the collected data to an observability backend, you can visualize trends, identify bottlenecks, and correlate runtime events with specific requests. OpenTelemetry captures:

- Traces with context propagation across services
- Metrics that are compatible with Prometheus
- Structured logs linked to trace context
- Runtime-level insights such as garbage collection, heap usage, and scheduler activity

With OpenTelemetry, you can connect these signals to analyze a single request end-to-end:

- Visualize execution timelines across internal services and external dependencies
- Measure latency of SQL queries and Redis commands per invocation
- Detect goroutine leaks, contention, and scheduling delays
- Understand memory allocation patterns and the impact of garbage collection
- Trace retry loops, queueing, and cascading delays in complex workflows

Because OpenTelemetry uses a standardized telemetry format, you can switch backends without changing your instrumentation. This unified approach provides insight into both runtime behavior and application-level activity, helping you see how the system actually performs under load.

## Set Up Monitoring in Go Applications using OpenTelemetry
Follow the below steps to monitor your Go application with OpenTelemetry, in this article we are using a sample [E-commerce Go Application](https://github.com/aayush-signoz/go-otel-ecommerce.git) demonstrating:

- **HTTP REST APIs** with [Gin](https://github.com/gin-gonic/gin)  
- **SQLite** as the database  
- **Redis** for caching  
- **OpenTelemetry** observability (Traces, Metrics, Logs)  

**Step 1: Prerequisites**

- **[Go installed (v1.20+)](https://go.dev/doc/install)**: Required to build and run the Go application.
- **[Git installed](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)**: Needed to clone the sample repository.
- **[SQLite installed](https://www.sqlite.org/download.html)**: The sample app uses SQLite as its database.
- **[Redis installed](https://redis.io/docs/latest/operate/oss_and_stack/install/archive/install-redis/)** *(optional but recommended)*: Enables caching and helps demonstrate Redis monitoring.

**Step 2: Get a sample Go app**

If you want to follow along with the tutorial, clone the `follow-along` branch below GitHub repository:

```bash
git clone -b follow-along https://github.com/aayush-signoz/go-otel-ecommerce.git
cd go-otel-ecommerce
```

**Step 3: Install dependencies**

Dependencies related to **[OpenTelemetry exporter](https://signoz.io/guides/opentelemetry-collector-vs-exporter/)** and SDK have to be installed first. 

Run the below commands:

```go
  go get go.opentelemetry.io/otel@latest
  go get go.opentelemetry.io/otel/attribute@latest
  go get go.opentelemetry.io/otel/metric@latest
  go get go.opentelemetry.io/otel/sdk/metric@latest
  go get go.opentelemetry.io/otel/sdk/resource@latest
  go get go.opentelemetry.io/otel/sdk/log@latest
  go get go.opentelemetry.io/otel/sdk/trace@latest
  go get go.opentelemetry.io/contrib/bridges/otellogrus@latest
  go get go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin@latest
  go get go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc@latest
  go get go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc@latest
  go get go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc@latest
```

- **`otel`**: Core OpenTelemetry API for Go  
- **`otel/attribute`**: Attribute key-value system used for spans, metrics, and logs  
- **`otel/metric`**: API for creating counters, histograms, gauges  
- **`otel/sdk/metric`**: SDK implementation for metrics, batching, exporting rules  
- **`otel/sdk/resource`**: Allows defining resources such as service.name, environment, version, etc.  
- **`otel/sdk/log`**: SDK implementation for OpenTelemetry logging  
- **`otel/sdk/trace`**: SDK implementation for tracing (span processors, samplers, exporters)  
- **`otellogrus`**: Bridge for converting Logrus structured logs into OTEL logs  
- **`otelgin`**: Gin middleware for automatic request tracing  
- **`otlpmetricgrpc`**: gRPC exporter that sends metrics to an OTLP backend  
- **`otlptracegrpc`**: gRPC exporter that sends traces to an OTLP backend  
- **`otlploggrpc`**: gRPC exporter that sends logs to an OTLP backend  

**Step 4: Instrument your Go application with OpenTelemetry**

Configure your application to send telemetry data to your observability backend. Start from initialize traces, metrics, and logs in `telemetry/otel.go`.

**1. Tracing (`initTracer`)**

```go
func InitTracer() func(context.Context) error {
	var opt otlptracegrpc.Option
	if config.Insecure == "false" {
		opt = otlptracegrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, ""))
	} else {
		opt = otlptracegrpc.WithInsecure()
	}

	exporter, err := otlptracegrpc.New(context.Background(),
		opt,
		otlptracegrpc.WithEndpoint(config.CollectorURL),
	)
	if err != nil {
		logrus.Fatalf("Failed to create trace exporter: %v", err)
	}

	provider := sdktrace.NewTracerProvider(
		sdktrace.WithResource(otelResource()),
		sdktrace.WithBatcher(exporter),
	)
	otel.SetTracerProvider(provider)
	return exporter.Shutdown
}
```
Above function initializes distributed tracing for the Golang application. It configures the trace exporter, sets up the tracing pipeline, and ensures all traces include service metadata for backend identification. It also handles secure/insecure modes depending on environment and returns a shutdown handler that flushes buffered spans on exit.
- `otlptracegrpc.New()`: creates a gRPC OTLP trace exporter for sending spans.
- **Security Mode**
  - When `config.Insecure == "false"`: communication is encrypted using TLS.
  - When true: it uses an unencrypted connection, suitable for local development.
- `sdktrace.NewTracerProvider()`: the core tracing system.
- `sdktrace.WithBatcher()`: batches spans for performance.
- `sdktrace.WithResource(otelResource())`: attaches service metadata (`service.name`, env, version, etc.).
- `otel.SetTracerProvider()`: registers the provider globally.
- `exporter.Shutdown`: ensures flushing of pending spans on shutdown.

With this in place, the application can produce traces, including nested spans,and request timings. Flamegraphs and timelines become available inside your tracing UI.

**2. Metrics (`initMeter`)**

```go
func InitMeter() func(context.Context) error {
	ctx := context.Background()
	var opt otlpmetricgrpc.Option
	if config.Insecure == "false" {
		opt = otlpmetricgrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, ""))
	} else {
		opt = otlpmetricgrpc.WithInsecure()
	}

	exporter, err := otlpmetricgrpc.New(ctx,
		opt,
		otlpmetricgrpc.WithEndpoint(config.CollectorURL),
	)
	if err != nil {
		logrus.Fatalf("Failed to create metric exporter: %v", err)
	}

	meterProvider := sdkmetric.NewMeterProvider(
		sdkmetric.WithResource(otelResource()),
		sdkmetric.WithReader(sdkmetric.NewPeriodicReader(exporter, sdkmetric.WithInterval(10*time.Second))),
	)
	otel.SetMeterProvider(meterProvider)

	meter := meterProvider.Meter(config.ServiceName)
	OrdersTotal, _ = meter.Int64Counter("orders_total")
	ProductOrderCounter, _ = meter.Int64Counter("product_order_total")
	HttpRequestCount, _ = meter.Int64Counter("http_request_count")
	HttpDurationBucket, _ = meter.Float64Histogram("http_request_duration")
	GoroutinesGauge, _ = meter.Int64ObservableGauge("go_goroutines")
	MemoryGauge, _ = meter.Int64ObservableGauge("go_memory_bytes")

	meter.RegisterCallback(func(ctx context.Context, o metric.Observer) error {
		var memStats runtime.MemStats
		runtime.ReadMemStats(&memStats)
		o.ObserveInt64(MemoryGauge, int64(memStats.Alloc))
		o.ObserveInt64(GoroutinesGauge, int64(runtime.NumGoroutine()))
		return nil
	}, MemoryGauge, GoroutinesGauge)
	otelruntime.Start(
		otelruntime.WithMeterProvider(meterProvider),
	)

	return meterProvider.Shutdown
}
```

Above function configures metric exporting for your Go service. It creates a gRPC-based OTLP metrics exporter, sets up a periodic reader to push data every 10s, registers application-level counters for business logic and HTTP requests, and exposes Go runtime telemetry such as memory allocation and goroutine count.

- `otlpmetricgrpc.New()`: initializes the OTLP metric exporter over gRPC.
- `sdkmetric.NewMeterProvider()`: central engine for emitting and managing metrics.
- `sdkmetric.WithPeriodicReader(...10s)`: batches and exports metrics every 10 seconds.
- `otel.SetMeterProvider()`: sets this meter provider as global for the entire process.
- Application Counters:
    - `orders_total`
    - `product_order_total`
    - `http_request_count`
    - `http_request_duration` (histogram)
- Runtime observable gauges:
    - `go_goroutines`
    - `go_memory_bytes`
- `meter.RegisterCallback()`: periodically samples runtime statistics and updates observable gauges.
- `Returns Shutdown()`: ensures clean flushing and shutdown of metric pipeline on program exit.

Above setup gives you both business-level telemetry and Go runtime behaviour in real time.

**3. Logging (`initLogger`)**

```go
func InitLogger() func(context.Context) error {
	var opt otlploggrpc.Option
	if config.Insecure == "false" {
		opt = otlploggrpc.WithTLSCredentials(credentials.NewClientTLSFromCert(nil, ""))
	} else {
		opt = otlploggrpc.WithInsecure()
	}

	exporter, err := otlploggrpc.New(context.Background(),
		opt,
		otlploggrpc.WithEndpoint(config.CollectorURL),
	)
	if err != nil {
		logrus.Fatalf("Failed to create log exporter: %v", err)
	}

	provider := otel_log.NewLoggerProvider(
		otel_log.WithResource(otelResource()),
		otel_log.WithProcessor(otel_log.NewBatchProcessor(exporter)),
	)

	logrus.AddHook(otellogrus.NewHook(config.ServiceName, otellogrus.WithLoggerProvider(provider)))
	return provider.Shutdown
}
```

Above function wires OpenTelemetry logging into the application. It configures an OTLP log exporter, creates a LoggerProvider that batches logs, and attaches a Logrus hook so log statements automatically include trace and span metadata.
- `otlploggrpc.New()`: creates gRPC exporter for logs.
- `Secure vs Insecure mode`: TLS for production, insecure for development.
- `otel_log.NewLoggerProvider()`: provider for structured logs.
- `otel_log.NewBatchProcessor()`: buffers log entries for performance.
- `logrus.AddHook(otellogrus.NewHook())`: attaches telemetry to logs using.
    - `trace_id`
    - `span_id`
    - `service.name`
- Works without modifying log statements: even plain `logrus.Info()` gets enriched.
- `Returns Shutdown()`: ensures logs are sent before exit.

Make sure to add the below imports in `otel.go` file to add all the relevant packages used:
```go
import (
	"runtime"
	"time"

	"github.com/sirupsen/logrus"
	"go.opentelemetry.io/contrib/bridges/otellogrus"
	otelruntime "go.opentelemetry.io/contrib/instrumentation/runtime"
	"go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc"
	"go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
	otel_log "go.opentelemetry.io/otel/sdk/log"
	sdkmetric "go.opentelemetry.io/otel/sdk/metric"
	sdktrace "go.opentelemetry.io/otel/sdk/trace"
	"google.golang.org/grpc/credentials"
)
```

After adding all of the above integrations in `otel.go`, it should look like [this](https://github.com/aayush-signoz/go-otel-ecommerce/blob/main/telemetry/otel.go). 

**Step 5: Initialize Tracing, Metrics & Logging in `main.go`**

We now wire everything together by initializing the telemetry components at the very start of the `main()` function in `main.go`. This ensures every request, log, and metric in the application is captured from the moment the service starts.

```go
	tracerShutdown := telemetry.InitTracer()
	defer tracerShutdown(context.Background())

	loggerShutdown := telemetry.InitLogger()
	defer loggerShutdown(context.Background())

	meterShutdown := telemetry.InitMeter()
	defer meterShutdown(context.Background())
```

Above code initializes OpenTelemetry tracing, logging, and metrics for the application, and ensures that all pending telemetry data is flushed properly when the program exits.

After adding all of the above integrations in `main.go`, it should look like [this](https://github.com/aayush-signoz/go-otel-ecommerce/blob/main/main.go). 

**Step 6: Declare environment variables for configuring OpenTelemetry**

Declare the following global variables in the `config/config.go` file. These will be used to configure OpenTelemetry and other service settings:

```go
var (
	ServiceName  = os.Getenv("SERVICE_NAME")
	CollectorURL = os.Getenv("OTEL_EXPORTER_OTLP_ENDPOINT")
	Insecure     = os.Getenv("INSECURE_MODE")
	RedisAddr    = os.Getenv("REDIS_ADDR")
)
```

| Variable        | Description                                               |
|-----------------|-----------------------------------------------------------|
| `ServiceName`   | Name of the service for tracing/logging/metrics |
| `CollectorURL`  | OTLP collector endpoint |
| `Insecure`      | `"true"` to skip TLS for OTLP, `"false"` for secure connection |
| `RedisAddr`     | Redis server address  |

Make sure to import the variables in all the paths needed, such as `telemetry/otel.go`, `main.go` and more. If you still face any dependencies related error, try running below to install all the required dependencies:
```bash
go mod tidy
```

**Step 7: Connecting a Backend - SigNoz**

SigNoz cloud is the easiest way to run SigNoz. You can sign upÂ [**here**](https://signoz.io/teams/)Â for a free account and get 30 days of unlimited access to all features.

![https://signoz.io/img/launch_week/try-signoz-cloud-blog-cta.png](https://signoz.io/img/launch_week/try-signoz-cloud-blog-cta.png)

After you sign up and verify your email, you will be provided with details of your SigNoz cloud instance. Once you set up your password and log in, you will be greeted with the following onboarding screen.

<figure data-zoomable align='center'>
<img src="https://signoz.io/img/opentelemetry/2024/06/onboarding-screen.webp" alt="Onboarding screen in SigNoz"/>
<figcaption><i>Onboarding screen in SigNoz</i></figcaption>
</figure>

Since we will be following instructions from the tutorial, you can skip onboarding by clicking on the SigNoz logo.

You will see the below screen:

<figure data-zoomable align='center'>
<img src="/img/blog/2024/12/services-tab.webp" alt="Services tab in SigNoz"/>
<figcaption><i>Services tab in SigNoz</i></figcaption>
</figure>

For sending data to SigNoz cloud, you will be needing details like [ingestion key](https://signoz.io/docs/ingestion/signoz-cloud/keys/) and region. You can find them underÂ **`Ingestion Settings`**Â underÂ **`Settings`**.

<figure data-zoomable align='center'>
<img src="/img/blog/2024/12/ingestion-settings.webp" alt="ingestion settings"/>
<figcaption><i>Ingestion details for your SigNoz cloud account</i></figcaption>
</figure>

**Step 8: Start Redis (required for app caching & Redis spans)**

This sample application uses Redis to cache frequently accessed data (like inventory and product availability), and these Redis operations also appear in your traces.

Create a new file named `docker-compose.yml` and add the below configuration:

```yml
version: '3.8'
services:
  redis:
    image: redis:7
    ports:
      - "6379:6379"
```

Run Redis using the below command:
```bash
docker-compose up -d
```

You should see:
```bash
redis:7   0.0.0.0:6379->6379/tcp
```

**Step 9: Running the Application**

Now that you have structured and instrumented your Go application with OpenTelemetry, you need to set some environment variables to send telemetry data to the SigNoz backend and run your application.

Run the following command:

```bash
SERVICE_NAME=goApp \ 
OTEL_SERVICE_NAME=goApp \ 
INSECURE_MODE=false \ 
REDIS_ADDR=<REDIS-ADDRESS> \  ## Default: 127.0.0.1:6379
OTEL_EXPORTER_OTLP_HEADERS="signoz-ingestion-key=<SIGNOZ-INGESTION-KEY>" \ 
OTEL_EXPORTER_OTLP_ENDPOINT=ingest.{region}.signoz.cloud:443 \ 
go run main.go
```

| Region | Endpoint |
|--------|----------|
| US | ingest.us.signoz.cloud:443 |
| IN | ingest.in.signoz.cloud:443 |
| EU | ingest.eu.signoz.cloud:443 |

### Once you run above command, your Go application will:

- Start the HTTP server on port `8080`.
- Send traces, metrics, and logs to SigNoz.
- Allow you to hit endpoints like `/products`, `/orders`, `/checkInventory`, `/cpuTest`, and `/concurrencyTest` with full observability.

**Your terminal should look like below:**
<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-app-running-monitoring.webp" alt="Go App started successfully"/>
<figcaption><i>App running</i></figcaption>
</figure>

You can find the complete code under the `main` branch of the sample [GitHub repository](https://github.com/aayush-signoz/go-otel-ecommerce/#main).

## Generate Telemetry Data from Application

To ensure telemetry data is generated and sent to SigNoz, interact with your Go ecommerce application by hitting its endpoints. You can do this by navigating to `http://localhost:8080` in your browser or using a tool like `curl` or Postman.

Refresh the endpoints multiple times to simulate load. Wait for 1â€“2 minutes, and the telemetry data (traces, metrics, and logs) will appear on your SigNoz dashboard, providing insights into your application's performance.

Below are examples to interact with the ecommerce app:

1. **Retrieve All Products**

Send a GET request to fetch the list of products:

```bash
curl http://localhost:8080/products
```

**Output:**

```json
{
  "products": [
    {"id":1,"name":"Book"},
    {"id":2,"name":"Laptop"},
    {"id":3,"name":"Phone"}
  ]
}
```

**Explanation:**
- Lists all available products in your ecommerce app.
- **Telemetry**: Each request will generate HTTP request count and duration metrics, along with traces for `/products` endpoint.

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-products.webp" alt="List all products"/>
<figcaption><i>Listed all products</i></figcaption>
</figure>

2. **Create a New Order**

Send a POST request to add a new order:

```bash
curl -X POST http://localhost:8080/orders \
     -H "Content-Type: application/json" \
     -d '{"product_name":"SigNoz", "quantity":1, "user_id":"user123"}'
```

**Output:**

```json
{"status":"order created"}
```

**Explanation:**
- Creates an order for the specified product.
- If the product does not exist, it will automatically be created in the database.
- **Telemetry**: Generates `orders_total`, `product_order_total`, HTTP request metrics, and traces containing span attributes:
  - `product.name`
  - `quantity`
- Also records logs showing `Order created`.

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-new-order.jpeg" alt="Create new order"/>
<figcaption><i>Created a new order</i></figcaption>
</figure>

3. **Check Inventory**

Send a GET request to check inventory status:

```bash
curl http://localhost:8080/checkInventory
```

**Output:**

```json
{
  "inventory_status": "in stock",
  "check_time_ms": 150
}
```

**Explanation:**
- Simulates inventory check with a random delay (100â€“400ms).
- **Telemetry**: Each request generates HTTP metrics, logs for inventory checks, and trace spans capturing the delay.

4. **CPU Load Test**

Send a GET request to simulate CPU-intensive work:

```bash
curl http://localhost:8080/cpuTest
```

**Output:**

```json
{"cpu_test_ms": 1200}
```

**Explanation:**
- Performs a heavy CPU calculation to simulate load.
- **Telemetry**: Observe CPU-bound spans and increased `goroutine/memory` metrics on your SigNoz dashboard.

5. **Concurrency Test**

Send a GET request to simulate high concurrency:

```bash
curl http://localhost:8080/concurrencyTest
```

**Output:**

```json
{"goroutines": 305}
```

**Explanation:**
- Spawns 300 goroutines that sleep for 200ms each, testing your application's concurrency handling.
- **Telemetry**: Observe goroutines gauge, memory usage, and HTTP metrics.

By performing these interactions, your application will generate telemetry data, which OpenTelemetry will process and forward to SigNoz for visualization and analysis. Refresh your SigNoz dashboard to observe the metrics, traces, and logs created during these operations!

## Monitor your Go application with SigNoz dashboards
With the above steps, you have instrumented your Go application with OpenTelemetry. OpenTelemetry sends the collected data to SigNoz which can be used to store it and visualize it. Letâ€™s see how SigNoz can help you monitor your Go application.

To see meaningful results in SigNoz, we need the application to actually process requests, otherwise thereâ€™s nothing for OpenTelemetry to capture. Since we are using a local sample app without real users, we need to generate synthetic traffic to simulate real-world API usage. Once the application is running, simply execute the provided **`generate_traffic.sh`** script to start producing telemetry data that you can immediately observe inside SigNoz using the below command:

```bash
chmod +x generate_traffic.sh
./generate_traffic.sh
```

The script repeatedly calls the /products, /orders, /checkInventory, /cpuTest, and /concurrencyTest endpoints, generating traces, metrics, and logs. This simulates real user activity, allowing SigNoz to visualize request latency, business metrics, and Go runtime behavior in real time.

Navigate to our SigNoz cloud account. On the service page, you should see your **`goApp`** service.

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/services-tab-go.webp" alt="Go app being monitored under services tab"/>
<figcaption><i>Your Go application being monitored on the SigNoz dashboard</i></figcaption>
</figure>

You can monitor application metrics like application latency, requests per second, error percentage, etc. with the Metrics tab of SigNoz.

Click on the goApp service and you should be redirected to the metrics page.

You can monitor application metrics like application latency, requests per second, error percentage, etc. with the **`Metrics`** tab of SigNoz. 

Click on the **`goApp`** service and you should be redirected to the metrics page.

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-service-metrics-page.webp" alt="Metrics from Go app"/>
<figcaption><i>Monitor your Go application metrics like application latency, requests per second, error percentage, etc.</i></figcaption>
</figure>

**1. Goroutines**
Shows the number of active goroutines in your application. Tracking goroutine counts helps identify concurrency spikes, leaks, or unusually high thread usage.

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-goroutines.webp" alt="Metrics from Go app"/>
<figcaption><i>Monitor your Go application metrics like application latency, requests per second, error percentage, etc.</i></figcaption>
</figure>

**2. Heap Memory Usage**
Reflects the bytes currently allocated on the Go heap. Monitoring this helps understand memory consumption patterns and the impact of garbage collection.

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-gomemory.webp" alt="Metrics from Go app"/>
<figcaption><i>Monitor your Go application metrics like application latency, requests per second, error percentage, etc.</i></figcaption>
</figure>

**3. Request Latency & Throughput**
- Latency: How long each request takes to complete. SigNoz shows percentiles like p50, p90, and p99 to highlight slow requests.
- Throughput: Number of requests served per second, helping understand application load handling.

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-latency.webp" alt="Metrics from Go app"/>
<figcaption><i>Monitor your Go application metrics like application latency, requests per second, error percentage, etc.</i></figcaption>
</figure>

**4. Business-Level Metrics**
Custom counters representing real user actions (e.g., orders_total, product_order_total) give visibility into business events alongside system metrics.

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-orders-total.webp" alt="Metrics from Go app"/>
<figcaption><i>Monitor your Go application metrics like application latency, requests per second, error percentage, etc.</i></figcaption>
</figure>

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-products-total.webp" alt="Metrics from Go app"/>
<figcaption><i>Monitor your Go application metrics like application latency, requests per second, error percentage, etc.</i></figcaption>
</figure>

**5. Tracing & Logs**
Traces visualize the execution path of requests across HTTP handlers, database calls, and Redis operations. Logs enriched with trace_id and span_id can be directly correlated to traces, simplifying debugging.
- Traces: Spans from otelgin.Middleware and manual spans in handlers (e.g., db_process_order)
- Logs: Structured logs enriched via otellogrus hook

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-trace.webp" alt="Metrics from Go app"/>
<figcaption><i>Monitor your Go application metrics like application latency, requests per second, error percentage, etc.</i></figcaption>
</figure>

<figure data-zoomable align='center'>
<img src="/img/blog/2025/11/go-monitoring-logs.webp" alt="Metrics from Go app"/>
<figcaption><i>Monitor your Go application metrics like application latency, requests per second, error percentage, etc.</i></figcaption>
</figure>

## Conclusion
By using OpenTelemetry libraries, you can instrument Go services with traces, metrics, and logs in a consistent way. Pairing that with SigNoz gives you the visibility needed to understand how your application behaves in production, track performance issues, and monitor user-impacting latency.

OpenTelemetry is supported by a strong community and continues to expand across frameworks and ecosystems. This makes it a reliable foundation for observability across distributed components and multi-service environments.

SigNoz provides an open-source tool that you can self-host or use through a managed option. To explore it further, you can check out the SigNoz GitHub repository ðŸ‘‡

![https://signoz.io/img/blog/common/signoz_github.webp](https://signoz.io/img/blog/common/signoz_github.webp)

If you want to read more about how to integrate Traces and Logs using Golang ðŸ‘‡

- [**Complete guide to implementing OpenTelemetry in Go applications**](https://signoz.io/opentelemetry/go/)
- [**Complete Guide to Logging in Go - Golang Log**](https://signoz.io/guides/golang-log/)

---

**Further Reading**

[**SigNoz - an open-source alternative to DataDog**](https://signoz.io/blog/open-source-datadog-alternative/)
